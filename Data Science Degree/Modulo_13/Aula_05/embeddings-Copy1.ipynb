{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb2481b",
   "metadata": {},
   "source": [
    "### Como utilizar ML com Dados Não Estruturais Textuais?\n",
    "\n",
    "* Técnicas de NLP preprocessam os dados para filtrarmos o que é mais relevante para a classificação \n",
    "\n",
    "![Title](../imgs/nlp-illustration.png)\n",
    "\n",
    "\n",
    "* A tokenização nos auxilia a dividir strings em palavras\n",
    "* Essas palavras serão as \"features\" que serão utilizadas para aprendizado de um modelo de Machine Learning\n",
    "* Um algoritmo de Machine Learning só entende números (atributos sempre numéricos), sendo assim, após a tokenização, precisamos converter nossas palavras para valores numéricos\n",
    "* O processo de transformação textual para base numérico denomina-se **bag of words**\n",
    "\n",
    "![Title](../imgs/1-Bag-of-words.png)\n",
    "\n",
    "* Técnicas como TF-IDF e CountVectorizer, apesar de eficientes, transformam o dataset em dado tabular\n",
    "  * Cada coluna é uma palavra\n",
    "  * Quando a palavra existir em uma dada amostra, substituimos o valor 0 pela frequência em que aquela palavra ocorre no texto\n",
    "  \n",
    "  <img src=\"https://www.researchgate.net/profile/Haider-Al-Khateeb/publication/291950178/figure/fig1/AS:330186932408324@1455734107458/Term-Frequency-Inverse-Document-Frequency-TF-IDF.png\" width=800>\n",
    "  \n",
    "* A consequencia disso é a perda da relação semântica nas frases\n",
    "* Perda de contexto das sentenças\n",
    "* Vocabulário imenso (maldição da dimensionalidade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409191a",
   "metadata": {},
   "source": [
    "### Word Embeddings to rescue!\n",
    "\n",
    "* Vetor multidimensional de palavras\n",
    "* Mapa de características com a frequência de palavras e considerações semânticas \n",
    "* Baseado em similaridade\n",
    "* Aprendizado por meio de redes neurais\n",
    "\n",
    "##### Como fazemos isso?\n",
    "* Dada uma palavra da frase\n",
    "* Pegar as palavras vizinhas \n",
    "* Rede Neural irá calcular a probabilidade dessa palavra vizinha ser a palavra a ser predita\n",
    "* Selecionamos as palavras vizinhas definindo uma \"janela\" ou \"kernel\"\n",
    "  * Janela = 2 considere duas palavras ao redor da palavra atual\n",
    "![Title](../imgs/training_data.png)\n",
    "\n",
    "* Criamos um corpus com as palavras organizadas de acordo com as janelas em torno do dataset.\n",
    "* Palavras que aparecerem juntas tem maior probabilidade de terem significado semântico\n",
    "  * Exemplo: \"soviética\" provavelmente terá uma semelhança com união\n",
    "  \n",
    " ![Title](../imgs/vetor_multi.png)\n",
    " \n",
    "[Leitura adicional](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/): funcionamento de uma rede neural para embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2cec0",
   "metadata": {},
   "source": [
    "### Arquiteturas Word Embeddings\n",
    "\n",
    "* Existem diferentes formas de gerar nossas embeddings, gerando determinadas arquiteturas previamente conhecidos como CBOW e SkipGram. Essas arquiteturas também são denominadas como tipo word2vec (palavra para vetor)\n",
    "\n",
    "* CBOW: predizer uma palavra baseado nas palavras vizinhas. Mais rápido, funciona melhor nas palavras mais frequentes.\n",
    "* Skip-Gram: predizer o contexto baseado nas palavras vizinhas. Funciona melhor com datasets menores\n",
    "<img src=\"https://leimao.github.io/images/article/2019-08-23-Word2Vec-Classic/word2vec.png\">\n",
    "* Ambas arquiteturas tem uma ambientação semelhante, modificando a forma como a saída e a entrada são organizadas\n",
    "* Exemplo: Hoje vai fazer sol pela manhã com pancadas de chuva à tarde\n",
    "* Utilizando CBOW\\\n",
    "![Title](../imgs/cbow.png)\n",
    "* Utilizando Skip-Gram\\\n",
    "![Title](../imgs/skip_gram.png)\n",
    "\n",
    "[Leitura adicional](https://www.tensorflow.org/tutorials/text/word2vec): transformando o corpus do CBOW/skip_gram em numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc7c86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/Faaeel06/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/Faaeel06/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Baixa as listas de stopwords e as tokenizações\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define as stopwords em inglês\n",
    "sw_english = set(stopwords.words('english'))\n",
    "\n",
    "# Instância o PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Carrega o conjunto de dados\n",
    "movies = pd.read_csv('../files/movies.csv', index_col = 0)\n",
    "\n",
    "# Retira uma amostra do conjunto de dados\n",
    "movies_sample = movies.sample(frac = 0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce7a0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14099</th>\n",
       "      <td>Winchester '73 was the film that moved Mann fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>\"Citizen X\" tells the story of \"The Butcher of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>After three hours in the Cinema hall,the stron...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>Really bad. Why anyone thinks this is a good f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Quite a lot has been said about this film and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9310</th>\n",
       "      <td>Bah. Another tired, desultory reworking of an ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9486</th>\n",
       "      <td>Alexander Nevsky (1938) is a brilliant piece o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24944</th>\n",
       "      <td>When I first popped in Happy Birthday to Me, I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36605</th>\n",
       "      <td>Well I watched this last night and the one thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30332</th>\n",
       "      <td>I'm sorry but this guy is not funny. I swear I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "14099  Winchester '73 was the film that moved Mann fr...      1\n",
       "4128   \"Citizen X\" tells the story of \"The Butcher of...      1\n",
       "718    After three hours in the Cinema hall,the stron...      0\n",
       "3343   Really bad. Why anyone thinks this is a good f...      0\n",
       "229    Quite a lot has been said about this film and ...      1\n",
       "...                                                  ...    ...\n",
       "9310   Bah. Another tired, desultory reworking of an ...      0\n",
       "9486   Alexander Nevsky (1938) is a brilliant piece o...      1\n",
       "24944  When I first popped in Happy Birthday to Me, I...      0\n",
       "36605  Well I watched this last night and the one thi...      0\n",
       "30332  I'm sorry but this guy is not funny. I swear I...      0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a55385c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um pipeline\n",
    "def preprocessing(string):\n",
    "    ###\n",
    "    # Deixa apenas elementos alfanuméricos\n",
    "    string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', string)\n",
    "    ###\n",
    "    # deixa todas as palavras minúsculas\n",
    "    string = string.lower()\n",
    "    ###\n",
    "    # tokenização\n",
    "    words = word_tokenize(string)\n",
    "    ###\n",
    "    # Remove Stopwords\n",
    "    filtered_words = []\n",
    "    for w in words:\n",
    "        if w not in sw_english:\n",
    "            filtered_words.append(w)\n",
    "    ###\n",
    "    # Aplica o Stemming\n",
    "    stem_words = []\n",
    "    for w in filtered_words:\n",
    "        s_words = stemmer.stem(w)\n",
    "        stem_words.append(s_words)\n",
    "    ###\n",
    "    # Retorna a lista de palavras pré-processadas\n",
    "    return stem_words\n",
    "\n",
    "# Aplica o preprocessing nas críticas de filmes\n",
    "movies_sample[\"filtered_words\"] = movies_sample['text'].apply(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "435df28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14099</th>\n",
       "      <td>Winchester '73 was the film that moved Mann fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>[winchest, 73, film, move, mann, b, movi, big,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>\"Citizen X\" tells the story of \"The Butcher of...</td>\n",
       "      <td>1</td>\n",
       "      <td>[citizen, x, tell, stori, butcher, rostov, nic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>After three hours in the Cinema hall,the stron...</td>\n",
       "      <td>0</td>\n",
       "      <td>[three, hour, cinema, hall, strongest, impress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>Really bad. Why anyone thinks this is a good f...</td>\n",
       "      <td>0</td>\n",
       "      <td>[realli, bad, anyon, think, good, film, let, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Quite a lot has been said about this film and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[quit, lot, said, film, landmark, import, form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9310</th>\n",
       "      <td>Bah. Another tired, desultory reworking of an ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bah, anoth, tire, desultori, rework, copyrigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9486</th>\n",
       "      <td>Alexander Nevsky (1938) is a brilliant piece o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[alexand, nevski, 1938, brilliant, piec, cinem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24944</th>\n",
       "      <td>When I first popped in Happy Birthday to Me, I...</td>\n",
       "      <td>0</td>\n",
       "      <td>[first, pop, happi, birthday, check, timer, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36605</th>\n",
       "      <td>Well I watched this last night and the one thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[well, watch, last, night, one, thing, make, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30332</th>\n",
       "      <td>I'm sorry but this guy is not funny. I swear I...</td>\n",
       "      <td>0</td>\n",
       "      <td>[sorri, guy, funni, swear, heard, heard, 4, ye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "14099  Winchester '73 was the film that moved Mann fr...      1   \n",
       "4128   \"Citizen X\" tells the story of \"The Butcher of...      1   \n",
       "718    After three hours in the Cinema hall,the stron...      0   \n",
       "3343   Really bad. Why anyone thinks this is a good f...      0   \n",
       "229    Quite a lot has been said about this film and ...      1   \n",
       "...                                                  ...    ...   \n",
       "9310   Bah. Another tired, desultory reworking of an ...      0   \n",
       "9486   Alexander Nevsky (1938) is a brilliant piece o...      1   \n",
       "24944  When I first popped in Happy Birthday to Me, I...      0   \n",
       "36605  Well I watched this last night and the one thi...      0   \n",
       "30332  I'm sorry but this guy is not funny. I swear I...      0   \n",
       "\n",
       "                                          filtered_words  \n",
       "14099  [winchest, 73, film, move, mann, b, movi, big,...  \n",
       "4128   [citizen, x, tell, stori, butcher, rostov, nic...  \n",
       "718    [three, hour, cinema, hall, strongest, impress...  \n",
       "3343   [realli, bad, anyon, think, good, film, let, a...  \n",
       "229    [quit, lot, said, film, landmark, import, form...  \n",
       "...                                                  ...  \n",
       "9310   [bah, anoth, tire, desultori, rework, copyrigh...  \n",
       "9486   [alexand, nevski, 1938, brilliant, piec, cinem...  \n",
       "24944  [first, pop, happi, birthday, check, timer, se...  \n",
       "36605  [well, watch, last, night, one, thing, make, c...  \n",
       "30332  [sorri, guy, funni, swear, heard, heard, 4, ye...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64609e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(movies_sample['filtered_words'])\n",
    "model = Word2Vec(\n",
    "        sentences = movies_sample['Filtered_words'],\n",
    "        vector_size= 100,\n",
    "        min_count= 1,\n",
    "        workers=8,\n",
    "        window=5,\n",
    "        sg=1, #### 1 para skip-gram, 0 para CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ceabde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bff7116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras 10 palavras: ['br', 'movi', 'film', 'one', 'like', 'time', 'good', 'make', 'get', 'charact'] \n",
      "\n",
      "\n",
      "Palavras similares a time: \n",
      " [('long', 0.8105802536010742), ('day', 0.7777706980705261), ('hour', 0.7667600512504578), ('rewatch', 0.7655228972434998), ('minut', 0.7577606439590454), ('spent', 0.7444809675216675), ('rememb', 0.7436185479164124), ('episod', 0.7387689352035522), ('spend', 0.7383964657783508), ('money', 0.7338274121284485)] \n",
      "\n",
      "\n",
      "Vetor para time: [-0.079018    0.23461457 -0.00283061 -0.09199482 -0.02540968 -0.02142855\n",
      "  0.04741906  0.16059865 -0.16620846 -0.12922662 -0.04197725 -0.01233783\n",
      " -0.10628784 -0.13860823  0.02164914 -0.08192834  0.06432919 -0.11845332\n",
      " -0.16049783 -0.11077248 -0.04205097  0.05053041  0.22148465 -0.1384986\n",
      " -0.1388212  -0.00153927 -0.06091898 -0.07174087 -0.0271758   0.01391348\n",
      "  0.03823535 -0.18785286 -0.01359395 -0.16428015 -0.04528658  0.16771941\n",
      "  0.05274598  0.08715541 -0.04662609  0.03533069  0.07265826 -0.09664395\n",
      " -0.00660456  0.05697339  0.00802042 -0.06430167  0.07152081  0.02521144\n",
      "  0.00365211  0.01518379  0.1307916  -0.07505522  0.02744037  0.18981455\n",
      " -0.02212484  0.06437939  0.0771752  -0.11180831 -0.16408068 -0.09282028\n",
      "  0.01012886 -0.03563554  0.14322013 -0.12303947 -0.15307525  0.08585471\n",
      "  0.0992059   0.05281087 -0.12123195  0.01486921  0.11934471  0.10206434\n",
      "  0.17270035  0.0885364   0.17163715  0.099046   -0.07995886  0.18789908\n",
      " -0.05337746 -0.00580289 -0.04258803 -0.00786234 -0.04220519 -0.00055114\n",
      " -0.01684023 -0.03646307  0.06755564  0.15655239  0.09967019  0.01843082\n",
      "  0.13511755  0.06199981 -0.00989875 -0.10540559  0.16161567  0.12565742\n",
      "  0.03703726 -0.13092273 -0.00330203 -0.1589831 ] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeiras 10 palavras:\", words[:10], \"\\n\\n\")\n",
    "print(\"Palavras similares a time: \\n\", model.wv.most_similar('time'), \"\\n\\n\")\n",
    "print('Vetor para time:', model.wv.get_vector('time', norm=True), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a9723fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9189932 , -0.30477402, -0.49083182, ..., -0.55766755,\n",
       "        -0.55950606,  0.36208224],\n",
       "       [-0.6183709 ,  0.5948241 ,  0.09102438, ..., -0.5801771 ,\n",
       "        -0.59225184, -0.6436222 ],\n",
       "       [-0.40518454,  0.6943868 , -0.04734575, ..., -1.0547723 ,\n",
       "        -0.42749998, -0.48876402],\n",
       "       ...,\n",
       "       [-0.04310771,  0.07033185, -0.00332596, ..., -0.04638806,\n",
       "         0.00865687,  0.03971612],\n",
       "       [-0.0370767 ,  0.06001392, -0.00220027, ..., -0.05178751,\n",
       "         0.01500952,  0.02939758],\n",
       "       [-0.02087016,  0.0354037 ,  0.01262193, ..., -0.02827134,\n",
       "         0.01310173,  0.01802594]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.wv[words]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3958863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7314542"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('time','second')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad6e00",
   "metadata": {},
   "source": [
    "### Treinamento de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdb9babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(movies_sample[\"filtered_words\"],\n",
    "                                                    movies_sample['label'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d49ec608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b90a209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12174/4006831418.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_vect = np.array([np.array([model.wv[i] for i in ls if i in words]) for ls in X_train])\n",
      "/tmp/ipykernel_12174/4006831418.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test_vect = np.array([np.array([model.wv[i] for i in ls if i in words]) for ls in X_test])\n"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.index_to_key)\n",
    "X_train_vect = np.array([np.array([model.wv[i] for i in ls if i in words]) for ls in X_train])\n",
    "X_test_vect = np.array([np.array([model.wv[i] for i in ls if i in words]) for ls in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b7ee2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[-0.22289124,  0.32768807, -0.09152499, ..., -0.46966347,\n",
       "               -0.07278646, -0.3690154 ],\n",
       "              [-0.21009372,  0.25849313,  0.13271204, ..., -0.34385616,\n",
       "                0.2316736 , -0.04882853],\n",
       "              [-0.11629928,  0.13996899,  0.07906803, ..., -0.1912492 ,\n",
       "                0.13345686, -0.02674216],\n",
       "              ...,\n",
       "              [-0.29447538,  0.19703929, -0.00130964, ..., -0.34480038,\n",
       "                0.11303307, -0.22679123],\n",
       "              [-0.04687925,  0.05918838,  0.03206633, ..., -0.0890332 ,\n",
       "                0.05313536, -0.00996448],\n",
       "              [-0.22310989,  0.08101626,  0.03454849, ..., -0.23812021,\n",
       "                0.00893255, -0.06481752]], dtype=float32)              ,\n",
       "       array([[-0.22794406, -0.21178688, -0.19366407, ..., -0.10597081,\n",
       "               -0.17505921, -0.00857241],\n",
       "              [-0.26960555, -0.24301074, -0.02814163, ..., -0.0238598 ,\n",
       "               -0.10245433, -0.02428582],\n",
       "              [-0.22289124,  0.32768807, -0.09152499, ..., -0.46966347,\n",
       "               -0.07278646, -0.3690154 ],\n",
       "              ...,\n",
       "              [-0.2034925 , -0.08377433, -0.2226026 , ..., -0.23117395,\n",
       "               -0.07541654, -0.00665261],\n",
       "              [-0.19865246, -0.46432278, -0.37398735, ..., -0.03027311,\n",
       "               -0.24058959,  0.07174757],\n",
       "              [-0.05568793, -0.37612027, -0.3424879 , ..., -0.2868476 ,\n",
       "               -0.11759558,  0.02767729]], dtype=float32)              ,\n",
       "       array([[-0.19900483,  0.16504808,  0.08804987, ..., -0.31316707,\n",
       "                0.18356518, -0.02217681],\n",
       "              [-0.18512774,  0.24984814,  0.06696102, ..., -0.11407404,\n",
       "               -0.07666318,  0.13507223],\n",
       "              [-0.14210492,  0.17792194,  0.06980891, ..., -0.24909613,\n",
       "                0.14406921, -0.07802337],\n",
       "              ...,\n",
       "              [-0.26927412,  0.2555046 ,  0.05361745, ..., -0.42318237,\n",
       "                0.20677647, -0.13089153],\n",
       "              [-0.21961789,  0.25826007,  0.13634947, ..., -0.35713267,\n",
       "                0.21952909, -0.0732467 ],\n",
       "              [-0.31029502, -0.20315316, -0.0996066 , ...,  0.04696035,\n",
       "               -0.07562575,  0.04073323]], dtype=float32)              ,\n",
       "       ...,\n",
       "       array([[-0.2332    ,  0.04857073, -0.01446955, ..., -0.37429744,\n",
       "                0.1062266 , -0.08100437],\n",
       "              [-0.23455662, -0.03350325,  0.03767342, ...,  0.20135668,\n",
       "               -0.2101443 , -0.02890405],\n",
       "              [-0.23311266,  0.22207567,  0.11126606, ..., -0.35779783,\n",
       "                0.18800068, -0.0584506 ],\n",
       "              ...,\n",
       "              [-0.42194915,  0.5862986 ,  0.13753134, ..., -0.5319915 ,\n",
       "                0.15228106, -0.2348361 ],\n",
       "              [-0.26783708,  0.29281664,  0.13202377, ..., -0.4692662 ,\n",
       "                0.27945027, -0.08272413],\n",
       "              [-0.29667184,  0.35573858,  0.19390167, ..., -0.47160882,\n",
       "                0.34006223, -0.03733596]], dtype=float32)              ,\n",
       "       array([[-0.22347954,  0.15521663, -0.00402761, ..., -0.4292918 ,\n",
       "                0.19493417, -0.11479054],\n",
       "              [-0.18799148, -0.10957558, -0.22058655, ..., -0.33145127,\n",
       "               -0.04333882, -0.23974505],\n",
       "              [-0.09150651,  0.06642881,  0.00469362, ..., -0.19987576,\n",
       "                0.0621671 , -0.02542395],\n",
       "              ...,\n",
       "              [-0.29777056,  0.25726566,  0.07807642, ..., -0.6068648 ,\n",
       "                0.3932113 , -0.15693597],\n",
       "              [-0.37989122,  0.39507213,  0.27116406, ..., -0.5456149 ,\n",
       "                0.46091634, -0.0243239 ],\n",
       "              [-0.02983074,  0.03814257,  0.02030813, ..., -0.05876178,\n",
       "                0.04994785, -0.01022936]], dtype=float32)              ,\n",
       "       array([[-0.07546762,  0.10859022,  0.06756869, ..., -0.13095935,\n",
       "                0.08908592, -0.00543501],\n",
       "              [-0.16802357,  0.20199078,  0.11194163, ..., -0.27814376,\n",
       "                0.17641154, -0.04151691],\n",
       "              [-0.24550341,  0.31545612,  0.17102204, ..., -0.42989406,\n",
       "                0.29896873, -0.08294645],\n",
       "              ...,\n",
       "              [-0.15379623,  0.17856938,  0.07886931, ..., -0.29702675,\n",
       "                0.16051033, -0.02833994],\n",
       "              [-0.29094896,  0.3982035 ,  0.22561985, ..., -0.47422144,\n",
       "                0.35156924, -0.0306598 ],\n",
       "              [-0.2242488 ,  0.25903228,  0.09703675, ..., -0.39464876,\n",
       "                0.22451538, -0.03224857]], dtype=float32)              ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f12eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_vect, y_train)\n",
    "y_pred = model.predict(X_test_vect)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
