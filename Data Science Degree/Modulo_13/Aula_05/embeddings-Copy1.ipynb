{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb2481b",
   "metadata": {},
   "source": [
    "### Como utilizar ML com Dados Não Estruturais Textuais?\n",
    "\n",
    "* Técnicas de NLP preprocessam os dados para filtrarmos o que é mais relevante para a classificação \n",
    "\n",
    "![Title](../imgs/nlp-illustration.png)\n",
    "\n",
    "\n",
    "* A tokenização nos auxilia a dividir strings em palavras\n",
    "* Essas palavras serão as \"features\" que serão utilizadas para aprendizado de um modelo de Machine Learning\n",
    "* Um algoritmo de Machine Learning só entende números (atributos sempre numéricos), sendo assim, após a tokenização, precisamos converter nossas palavras para valores numéricos\n",
    "* O processo de transformação textual para base numérico denomina-se **bag of words**\n",
    "\n",
    "![Title](../imgs/1-Bag-of-words.png)\n",
    "\n",
    "* Técnicas como TF-IDF e CountVectorizer, apesar de eficientes, transformam o dataset em dado tabular\n",
    "  * Cada coluna é uma palavra\n",
    "  * Quando a palavra existir em uma dada amostra, substituimos o valor 0 pela frequência em que aquela palavra ocorre no texto\n",
    "  \n",
    "  <img src=\"https://www.researchgate.net/profile/Haider-Al-Khateeb/publication/291950178/figure/fig1/AS:330186932408324@1455734107458/Term-Frequency-Inverse-Document-Frequency-TF-IDF.png\" width=800>\n",
    "  \n",
    "* A consequencia disso é a perda da relação semântica nas frases\n",
    "* Perda de contexto das sentenças\n",
    "* Vocabulário imenso (maldição da dimensionalidade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409191a",
   "metadata": {},
   "source": [
    "### Word Embeddings to rescue!\n",
    "\n",
    "* Vetor multidimensional de palavras\n",
    "* Mapa de características com a frequência de palavras e considerações semânticas \n",
    "* Baseado em similaridade\n",
    "* Aprendizado por meio de redes neurais\n",
    "\n",
    "##### Como fazemos isso?\n",
    "* Dada uma palavra da frase\n",
    "* Pegar as palavras vizinhas \n",
    "* Rede Neural irá calcular a probabilidade dessa palavra vizinha ser a palavra a ser predita\n",
    "* Selecionamos as palavras vizinhas definindo uma \"janela\" ou \"kernel\"\n",
    "  * Janela = 2 considere duas palavras ao redor da palavra atual\n",
    "![Title](../imgs/training_data.png)\n",
    "\n",
    "* Criamos um corpus com as palavras organizadas de acordo com as janelas em torno do dataset.\n",
    "* Palavras que aparecerem juntas tem maior probabilidade de terem significado semântico\n",
    "  * Exemplo: \"soviética\" provavelmente terá uma semelhança com união\n",
    "  \n",
    " ![Title](../imgs/vetor_multi.png)\n",
    " \n",
    "[Leitura adicional](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/): funcionamento de uma rede neural para embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2cec0",
   "metadata": {},
   "source": [
    "### Arquiteturas Word Embeddings\n",
    "\n",
    "* Existem diferentes formas de gerar nossas embeddings, gerando determinadas arquiteturas previamente conhecidos como CBOW e SkipGram. Essas arquiteturas também são denominadas como tipo word2vec (palavra para vetor)\n",
    "\n",
    "* CBOW: predizer uma palavra baseado nas palavras vizinhas. Mais rápido, funciona melhor nas palavras mais frequentes.\n",
    "* Skip-Gram: predizer o contexto baseado nas palavras vizinhas. Funciona melhor com datasets menores\n",
    "<img src=\"https://leimao.github.io/images/article/2019-08-23-Word2Vec-Classic/word2vec.png\">\n",
    "* Ambas arquiteturas tem uma ambientação semelhante, modificando a forma como a saída e a entrada são organizadas\n",
    "* Exemplo: Hoje vai fazer sol pela manhã com pancadas de chuva à tarde\n",
    "* Utilizando CBOW\\\n",
    "![Title](../imgs/cbow.png)\n",
    "* Utilizando Skip-Gram\\\n",
    "![Title](../imgs/skip_gram.png)\n",
    "\n",
    "[Leitura adicional](https://www.tensorflow.org/tutorials/text/word2vec): transformando o corpus do CBOW/skip_gram em numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc7c86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/Faaeel06/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/Faaeel06/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Baixa as listas de stopwords e as tokenizações\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define as stopwords em inglês\n",
    "sw_english = set(stopwords.words('english'))\n",
    "\n",
    "# Instância o PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Carrega o conjunto de dados\n",
    "movies = pd.read_csv('../files/movies.csv', index_col = 0)\n",
    "\n",
    "# Retira uma amostra do conjunto de dados\n",
    "movies_sample = movies.sample(frac = 0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55385c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um pipeline\n",
    "def preprocessing(string):\n",
    "    ###\n",
    "    # Deixa apenas elementos alfanuméricos\n",
    "    string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', string)\n",
    "    ###\n",
    "    # deixa todas as palavras minúsculas\n",
    "    string = string.lower()\n",
    "    ###\n",
    "    # tokenização\n",
    "    words = word_tokenize(string)\n",
    "    ###\n",
    "    # Remove Stopwords\n",
    "    filtered_words = []\n",
    "    for w in words:\n",
    "        if w not in sw_english:\n",
    "            filtered_words.append(w)\n",
    "    ###\n",
    "    # Aplica o Stemming\n",
    "    stem_words = []\n",
    "    for w in filtered_words:\n",
    "        s_words = stemmer.stem(w)\n",
    "        stem_words.append(s_words)\n",
    "    ###\n",
    "    # Retorna a lista de palavras pré-processadas\n",
    "    return stem_words\n",
    "\n",
    "# Aplica o preprocessing nas críticas de filmes\n",
    "movies_sample[\"filtered_words\"] = movies_sample['text'].apply(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435df28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>My only reason registering to this site was fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[reason, regist, site, opportun, write, commen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8555</th>\n",
       "      <td>I really enjoyed this old black and white talk...</td>\n",
       "      <td>1</td>\n",
       "      <td>[realli, enjoy, old, black, white, talki, firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>Time travel is theoretical, so I can give the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[time, travel, theoret, give, script, leeway, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35024</th>\n",
       "      <td>The Last Hunt is one of the few westerns ever ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[last, hunt, one, western, ever, made, deal, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>It is disappointing to see as talented an acto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[disappoint, see, talent, actor, amitabh, bach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Dr. Mordrid, what can I say? Jeffrey Combs has...</td>\n",
       "      <td>1</td>\n",
       "      <td>[dr, mordrid, say, jeffrey, comb, done, br, br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15420</th>\n",
       "      <td>This show has shown it's true colors now that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[show, shown, true, color, democrat, power, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>This is an excellent but hard to find trippy W...</td>\n",
       "      <td>1</td>\n",
       "      <td>[excel, hard, find, trippi, world, war, spi, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23724</th>\n",
       "      <td>I saw this film over Christmas, and what a gre...</td>\n",
       "      <td>1</td>\n",
       "      <td>[saw, film, christma, great, film, tell, stori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>A friend brought me this movie and at first I ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[friend, brought, movi, first, hesit, pace, mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "850    My only reason registering to this site was fo...      0   \n",
       "8555   I really enjoyed this old black and white talk...      1   \n",
       "2150   Time travel is theoretical, so I can give the ...      0   \n",
       "35024  The Last Hunt is one of the few westerns ever ...      1   \n",
       "3795   It is disappointing to see as talented an acto...      0   \n",
       "...                                                  ...    ...   \n",
       "432    Dr. Mordrid, what can I say? Jeffrey Combs has...      1   \n",
       "15420  This show has shown it's true colors now that ...      0   \n",
       "317    This is an excellent but hard to find trippy W...      1   \n",
       "23724  I saw this film over Christmas, and what a gre...      1   \n",
       "3989   A friend brought me this movie and at first I ...      1   \n",
       "\n",
       "                                          filtered_words  \n",
       "850    [reason, regist, site, opportun, write, commen...  \n",
       "8555   [realli, enjoy, old, black, white, talki, firs...  \n",
       "2150   [time, travel, theoret, give, script, leeway, ...  \n",
       "35024  [last, hunt, one, western, ever, made, deal, b...  \n",
       "3795   [disappoint, see, talent, actor, amitabh, bach...  \n",
       "...                                                  ...  \n",
       "432    [dr, mordrid, say, jeffrey, comb, done, br, br...  \n",
       "15420  [show, shown, true, color, democrat, power, ne...  \n",
       "317    [excel, hard, find, trippi, world, war, spi, t...  \n",
       "23724  [saw, film, christma, great, film, tell, stori...  \n",
       "3989   [friend, brought, movi, first, hesit, pace, mo...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad6e00",
   "metadata": {},
   "source": [
    "### Treinamento de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9babb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
