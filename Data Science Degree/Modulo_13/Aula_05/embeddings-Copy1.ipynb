{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb2481b",
   "metadata": {},
   "source": [
    "### Como utilizar ML com Dados Não Estruturais Textuais?\n",
    "\n",
    "* Técnicas de NLP preprocessam os dados para filtrarmos o que é mais relevante para a classificação \n",
    "\n",
    "![Title](../imgs/nlp-illustration.png)\n",
    "\n",
    "\n",
    "* A tokenização nos auxilia a dividir strings em palavras\n",
    "* Essas palavras serão as \"features\" que serão utilizadas para aprendizado de um modelo de Machine Learning\n",
    "* Um algoritmo de Machine Learning só entende números (atributos sempre numéricos), sendo assim, após a tokenização, precisamos converter nossas palavras para valores numéricos\n",
    "* O processo de transformação textual para base numérico denomina-se **bag of words**\n",
    "\n",
    "![Title](../imgs/1-Bag-of-words.png)\n",
    "\n",
    "* Técnicas como TF-IDF e CountVectorizer, apesar de eficientes, transformam o dataset em dado tabular\n",
    "  * Cada coluna é uma palavra\n",
    "  * Quando a palavra existir em uma dada amostra, substituimos o valor 0 pela frequência em que aquela palavra ocorre no texto\n",
    "  \n",
    "  <img src=\"https://www.researchgate.net/profile/Haider-Al-Khateeb/publication/291950178/figure/fig1/AS:330186932408324@1455734107458/Term-Frequency-Inverse-Document-Frequency-TF-IDF.png\" width=800>\n",
    "  \n",
    "* A consequencia disso é a perda da relação semântica nas frases\n",
    "* Perda de contexto das sentenças\n",
    "* Vocabulário imenso (maldição da dimensionalidade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409191a",
   "metadata": {},
   "source": [
    "### Word Embeddings to rescue!\n",
    "\n",
    "* Vetor multidimensional de palavras\n",
    "* Mapa de características com a frequência de palavras e considerações semânticas \n",
    "* Baseado em similaridade\n",
    "* Aprendizado por meio de redes neurais\n",
    "\n",
    "##### Como fazemos isso?\n",
    "* Dada uma palavra da frase\n",
    "* Pegar as palavras vizinhas \n",
    "* Rede Neural irá calcular a probabilidade dessa palavra vizinha ser a palavra a ser predita\n",
    "* Selecionamos as palavras vizinhas definindo uma \"janela\" ou \"kernel\"\n",
    "  * Janela = 2 considere duas palavras ao redor da palavra atual\n",
    "![Title](../imgs/training_data.png)\n",
    "\n",
    "* Criamos um corpus com as palavras organizadas de acordo com as janelas em torno do dataset.\n",
    "* Palavras que aparecerem juntas tem maior probabilidade de terem significado semântico\n",
    "  * Exemplo: \"soviética\" provavelmente terá uma semelhança com união\n",
    "  \n",
    " ![Title](../imgs/vetor_multi.png)\n",
    " \n",
    "[Leitura adicional](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/): funcionamento de uma rede neural para embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2cec0",
   "metadata": {},
   "source": [
    "### Arquiteturas Word Embeddings\n",
    "\n",
    "* Existem diferentes formas de gerar nossas embeddings, gerando determinadas arquiteturas previamente conhecidos como CBOW e SkipGram. Essas arquiteturas também são denominadas como tipo word2vec (palavra para vetor)\n",
    "\n",
    "* CBOW: predizer uma palavra baseado nas palavras vizinhas. Mais rápido, funciona melhor nas palavras mais frequentes.\n",
    "* Skip-Gram: predizer o contexto baseado nas palavras vizinhas. Funciona melhor com datasets menores\n",
    "<img src=\"https://leimao.github.io/images/article/2019-08-23-Word2Vec-Classic/word2vec.png\">\n",
    "* Ambas arquiteturas tem uma ambientação semelhante, modificando a forma como a saída e a entrada são organizadas\n",
    "* Exemplo: Hoje vai fazer sol pela manhã com pancadas de chuva à tarde\n",
    "* Utilizando CBOW\\\n",
    "![Title](../imgs/cbow.png)\n",
    "* Utilizando Skip-Gram\\\n",
    "![Title](../imgs/skip_gram.png)\n",
    "\n",
    "[Leitura adicional](https://www.tensorflow.org/tutorials/text/word2vec): transformando o corpus do CBOW/skip_gram em numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc7c86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/Faaeel06/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/Faaeel06/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Baixa as listas de stopwords e as tokenizações\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define as stopwords em inglês\n",
    "sw_english = set(stopwords.words('english'))\n",
    "\n",
    "# Instância o PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Carrega o conjunto de dados\n",
    "movies = pd.read_csv('../files/movies.csv', index_col = 0)\n",
    "\n",
    "# Retira uma amostra do conjunto de dados\n",
    "movies_sample = movies.sample(frac = 0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce7a0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>I was blown away when I saw \"The Best Years of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23887</th>\n",
       "      <td>The revisionist history -- making the evil Mar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>\"The 40 Year Old Virgin\" exists in a world I d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34861</th>\n",
       "      <td>This movie was very funny, I couldn't stop smi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19491</th>\n",
       "      <td>It was a doubly interesting experience. For so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5296</th>\n",
       "      <td>The original Road House was a classic cheesy 8...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13522</th>\n",
       "      <td>Noting the cast, I recently watched this movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33354</th>\n",
       "      <td>This is an hilarious movie. One of the very be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23479</th>\n",
       "      <td>I wasn't so impressed with this film, finding ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34311</th>\n",
       "      <td>Starting with a \"My Name is Joe\" like scene in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "8756   I was blown away when I saw \"The Best Years of...      1\n",
       "23887  The revisionist history -- making the evil Mar...      0\n",
       "17998  \"The 40 Year Old Virgin\" exists in a world I d...      0\n",
       "34861  This movie was very funny, I couldn't stop smi...      1\n",
       "19491  It was a doubly interesting experience. For so...      1\n",
       "...                                                  ...    ...\n",
       "5296   The original Road House was a classic cheesy 8...      0\n",
       "13522  Noting the cast, I recently watched this movie...      0\n",
       "33354  This is an hilarious movie. One of the very be...      1\n",
       "23479  I wasn't so impressed with this film, finding ...      0\n",
       "34311  Starting with a \"My Name is Joe\" like scene in...      1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55385c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um pipeline\n",
    "def preprocessing(string):\n",
    "    ###\n",
    "    # Deixa apenas elementos alfanuméricos\n",
    "    string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', string)\n",
    "    ###\n",
    "    # deixa todas as palavras minúsculas\n",
    "    string = string.lower()\n",
    "    ###\n",
    "    # tokenização\n",
    "    words = word_tokenize(string)\n",
    "    ###\n",
    "    # Remove Stopwords\n",
    "    filtered_words = []\n",
    "    for w in words:\n",
    "        if w not in sw_english:\n",
    "            filtered_words.append(w)\n",
    "    ###\n",
    "    # Aplica o Stemming\n",
    "    stem_words = []\n",
    "    for w in filtered_words:\n",
    "        s_words = stemmer.stem(w)\n",
    "        stem_words.append(s_words)\n",
    "    ###\n",
    "    # Retorna a lista de palavras pré-processadas\n",
    "    return stem_words\n",
    "\n",
    "# Aplica o preprocessing nas críticas de filmes\n",
    "movies_sample[\"filtered_words\"] = movies_sample['text'].apply(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435df28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>I was blown away when I saw \"The Best Years of...</td>\n",
       "      <td>1</td>\n",
       "      <td>[blown, away, saw, best, year, live, act, scri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23887</th>\n",
       "      <td>The revisionist history -- making the evil Mar...</td>\n",
       "      <td>0</td>\n",
       "      <td>[revisionist, histori, make, evil, marqui, de,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>\"The 40 Year Old Virgin\" exists in a world I d...</td>\n",
       "      <td>0</td>\n",
       "      <td>[40, year, old, virgin, exist, world, understa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34861</th>\n",
       "      <td>This movie was very funny, I couldn't stop smi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[movi, funni, stop, smile, watch, alreadi, wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19491</th>\n",
       "      <td>It was a doubly interesting experience. For so...</td>\n",
       "      <td>1</td>\n",
       "      <td>[doubli, interest, experi, reason, greatest, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5296</th>\n",
       "      <td>The original Road House was a classic cheesy 8...</td>\n",
       "      <td>0</td>\n",
       "      <td>[origin, road, hous, classic, cheesi, 80, movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13522</th>\n",
       "      <td>Noting the cast, I recently watched this movie...</td>\n",
       "      <td>0</td>\n",
       "      <td>[note, cast, recent, watch, movi, tcm, hope, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33354</th>\n",
       "      <td>This is an hilarious movie. One of the very be...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hilari, movi, one, best, thing, qualiti, perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23479</th>\n",
       "      <td>I wasn't so impressed with this film, finding ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[impress, film, find, quit, tiresom, plain, pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34311</th>\n",
       "      <td>Starting with a \"My Name is Joe\" like scene in...</td>\n",
       "      <td>1</td>\n",
       "      <td>[start, name, joe, like, scene, alcohol, anony...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "8756   I was blown away when I saw \"The Best Years of...      1   \n",
       "23887  The revisionist history -- making the evil Mar...      0   \n",
       "17998  \"The 40 Year Old Virgin\" exists in a world I d...      0   \n",
       "34861  This movie was very funny, I couldn't stop smi...      1   \n",
       "19491  It was a doubly interesting experience. For so...      1   \n",
       "...                                                  ...    ...   \n",
       "5296   The original Road House was a classic cheesy 8...      0   \n",
       "13522  Noting the cast, I recently watched this movie...      0   \n",
       "33354  This is an hilarious movie. One of the very be...      1   \n",
       "23479  I wasn't so impressed with this film, finding ...      0   \n",
       "34311  Starting with a \"My Name is Joe\" like scene in...      1   \n",
       "\n",
       "                                          filtered_words  \n",
       "8756   [blown, away, saw, best, year, live, act, scri...  \n",
       "23887  [revisionist, histori, make, evil, marqui, de,...  \n",
       "17998  [40, year, old, virgin, exist, world, understa...  \n",
       "34861  [movi, funni, stop, smile, watch, alreadi, wat...  \n",
       "19491  [doubli, interest, experi, reason, greatest, s...  \n",
       "...                                                  ...  \n",
       "5296   [origin, road, hous, classic, cheesi, 80, movi...  \n",
       "13522  [note, cast, recent, watch, movi, tcm, hope, a...  \n",
       "33354  [hilari, movi, one, best, thing, qualiti, perf...  \n",
       "23479  [impress, film, find, quit, tiresom, plain, pl...  \n",
       "34311  [start, name, joe, like, scene, alcohol, anony...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64609e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(movies_sample['filtered_words'])\n",
    "model = Word2Vec(\n",
    "        sentences = movies_sample['filtered_words'],\n",
    "        vector_size= 100,\n",
    "        min_count= 1,\n",
    "        workers=8,\n",
    "        window=5,\n",
    "        sg=1, #### 1 para skip-gram, 0 para CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ceabde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bff7116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras 10 palavras: ['br', 'movi', 'film', 'one', 'like', 'good', 'time', 'make', 'get', 'see'] \n",
      "\n",
      "\n",
      "Palavras similares a time: \n",
      " [('celluloid', 0.7939850091934204), ('preciou', 0.7900879383087158), ('split', 0.7758944034576416), ('afterward', 0.7690925598144531), ('thru', 0.7686951160430908), ('nearli', 0.7666786909103394), ('repeat', 0.7666296362876892), ('id', 0.7649783492088318), ('opportun', 0.7625721096992493), ('tomorrow', 0.7605178952217102)] \n",
      "\n",
      "\n",
      "Vetor para time: [ 0.02527022  0.04148626  0.05303635  0.07578712  0.00516384 -0.19452904\n",
      "  0.11483211  0.17773536 -0.04306247 -0.15226576  0.00239717 -0.17972405\n",
      " -0.10817587  0.17709993 -0.05973883  0.01985734  0.08162588  0.02268315\n",
      " -0.17652161 -0.26361853  0.11976416 -0.03794185  0.07819799 -0.02744842\n",
      "  0.02710967  0.08144224 -0.12959158 -0.01833709 -0.14740601  0.04021579\n",
      "  0.13443269  0.12660268  0.05430776 -0.03120637 -0.06164306  0.01505889\n",
      "  0.02480303 -0.09145157 -0.19220999 -0.07239845  0.01723683 -0.0270832\n",
      "  0.0260359  -0.08510366  0.01833193  0.002247   -0.11322127 -0.04122269\n",
      " -0.05046358  0.07161958  0.06005195 -0.04579738 -0.00190801  0.16431698\n",
      " -0.16343091  0.07357991 -0.06115472 -0.16357134 -0.04971763 -0.0444182\n",
      " -0.01349536 -0.06761281 -0.0049788  -0.03216347 -0.1818594   0.08113918\n",
      " -0.0855183   0.12168802 -0.24013238 -0.03899192 -0.06125878  0.11635005\n",
      "  0.0940742   0.08307982  0.09367353 -0.06829941  0.09866556  0.01499607\n",
      " -0.15740712  0.05137305 -0.0113591  -0.03646543 -0.07363151  0.18542725\n",
      "  0.02001812 -0.11496539  0.17192097  0.04334671 -0.04219949  0.07081835\n",
      " -0.05073413 -0.00148929 -0.08061526  0.04787669  0.14077696  0.1921806\n",
      "  0.1319636  -0.07500435 -0.06416668  0.0155751 ] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeiras 10 palavras:\", words[:10], \"\\n\\n\")\n",
    "print(\"Palavras similares a time: \\n\", model.wv.most_similar('time'), \"\\n\\n\")\n",
    "print('Vetor para time:', model.wv.get_vector('time', norm=True), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a9723fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.91651925e-01,  1.36527941e-01,  1.95569545e-01, ...,\n",
       "        -1.15626596e-01,  2.05697529e-02,  2.71272480e-01],\n",
       "       [-5.02077758e-01,  8.72410834e-02,  9.16646868e-02, ...,\n",
       "        -2.41754025e-01, -1.79974675e-01,  7.60941580e-02],\n",
       "       [-2.98758149e-01,  2.72341847e-01,  6.98877848e-04, ...,\n",
       "        -1.81508362e-01, -2.36868218e-01,  1.29560307e-01],\n",
       "       ...,\n",
       "       [-1.00133777e-01,  1.35118052e-01,  1.05701238e-01, ...,\n",
       "        -6.13865927e-02,  7.07681244e-03,  2.64676902e-02],\n",
       "       [-7.79602155e-02,  1.13227785e-01,  9.12993103e-02, ...,\n",
       "        -3.19390856e-02,  4.47358529e-04, -6.22297055e-04],\n",
       "       [-5.76751120e-02,  9.26765278e-02,  6.27294704e-02, ...,\n",
       "        -3.61044928e-02, -1.37572829e-03,  1.18741812e-02]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.wv[words]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3958863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.684262"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('time','second')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad6e00",
   "metadata": {},
   "source": [
    "### Treinamento de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb9babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(movies_sample[\"filtered_words\"],\n",
    "                                                    movies_sample['label'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d49ec608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b90a209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18831/4006831418.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_vect = np.array([np.array([model.wv[i] for i in ls if i in words]) for ls in X_train])\n",
      "/tmp/ipykernel_18831/4006831418.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test_vect = np.array([np.array([model.wv[i] for i in ls if i in words]) for ls in X_test])\n"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.index_to_key)\n",
    "X_train_vect = np.array([np.array([model.wv[i] for i in ls if i in words]) for ls in X_train])\n",
    "X_test_vect = np.array([np.array([model.wv[i] for i in ls if i in words]) for ls in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b7ee2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[-0.6228106 ,  0.59790635,  0.2813161 , ..., -1.0105655 ,\n",
       "               -0.506629  ,  0.29375982],\n",
       "              [-0.15514997,  0.25937256,  0.01537842, ..., -0.20945041,\n",
       "               -0.00947169, -0.0196006 ],\n",
       "              [-0.15151815,  0.6922427 , -0.07741792, ..., -0.25776216,\n",
       "               -0.24856935,  0.04601016],\n",
       "              ...,\n",
       "              [-0.44970167,  0.05530951,  0.22182508, ..., -0.29104635,\n",
       "               -0.69238734,  0.06700387],\n",
       "              [-0.41790688, -0.01518096,  0.24296077, ..., -0.5936734 ,\n",
       "               -0.18817015, -0.01835886],\n",
       "              [-0.7698827 , -0.17955036,  0.14720777, ..., -0.8593956 ,\n",
       "               -0.7365144 , -0.14187057]], dtype=float32)              ,\n",
       "       array([[-0.3166941 ,  0.3639355 ,  0.13409203, ..., -0.53619903,\n",
       "               -0.04459017,  0.01239145],\n",
       "              [-0.6228106 ,  0.59790635,  0.2813161 , ..., -1.0105655 ,\n",
       "               -0.506629  ,  0.29375982],\n",
       "              [-0.12308968,  0.19880015,  0.00616722, ..., -0.18366183,\n",
       "                0.08507742, -0.03633595],\n",
       "              ...,\n",
       "              [-0.2802666 ,  0.4642219 ,  0.07697774, ..., -0.39861804,\n",
       "               -0.28376806,  0.04285256],\n",
       "              [-0.0441509 ,  0.10733322,  0.00295225, ..., -0.07179929,\n",
       "                0.02386544,  0.00390593],\n",
       "              [-0.31020662,  0.26778847,  0.6446324 , ..., -0.789071  ,\n",
       "               -0.3388523 ,  0.40037644]], dtype=float32)              ,\n",
       "       array([[-0.33808282,  0.25545058,  0.14793105, ..., -0.6459025 ,\n",
       "               -0.09342062, -0.07708351],\n",
       "              [-0.3058376 ,  0.49586838,  0.10055827, ..., -0.5952193 ,\n",
       "               -0.16441464,  0.0106659 ],\n",
       "              [-0.4123506 , -0.5230165 , -0.22960004, ..., -0.23655537,\n",
       "                0.02856367, -0.32391393],\n",
       "              ...,\n",
       "              [-0.01159596,  0.30005476,  0.3044241 , ..., -0.20959492,\n",
       "               -0.09940242, -0.01063405],\n",
       "              [-0.4074909 ,  0.48128474,  0.19128612, ..., -0.76424223,\n",
       "               -0.38302872,  0.11733654],\n",
       "              [-0.43079403,  0.38215524,  0.35234   , ..., -1.025558  ,\n",
       "               -0.63742423,  0.35855418]], dtype=float32)              ,\n",
       "       ...,\n",
       "       array([[-0.20679931,  0.60784245, -0.06818396, ..., -0.2987248 ,\n",
       "                0.29976365, -0.07729384],\n",
       "              [-0.08711228,  0.1549326 ,  0.01507153, ..., -0.12612294,\n",
       "                0.06490348, -0.01642651],\n",
       "              [-0.6554628 , -0.36949173,  0.49846965, ..., -0.5433603 ,\n",
       "               -0.2772468 , -0.06814221],\n",
       "              ...,\n",
       "              [-0.30822456,  0.48264185,  0.15264082, ..., -0.5957978 ,\n",
       "               -0.1904974 ,  0.04453438],\n",
       "              [-0.13932028,  0.25450197,  0.01133375, ..., -0.18645689,\n",
       "                0.0741432 , -0.03390694],\n",
       "              [-0.8355088 ,  1.376504  , -0.01870243, ..., -0.9412685 ,\n",
       "               -0.7294575 , -0.57582533]], dtype=float32)              ,\n",
       "       array([[-1.8088505e-01,  3.8458019e-01,  3.0198868e-02, ...,\n",
       "               -2.2791362e-01,  5.5618107e-02, -2.9481482e-02],\n",
       "              [-1.4477880e-01,  3.5826656e-01, -5.2907458e-04, ...,\n",
       "               -1.4387515e-01,  9.7217873e-02, -1.9361584e-02],\n",
       "              [-1.5514997e-01,  2.5937256e-01,  1.5378425e-02, ...,\n",
       "               -2.0945041e-01, -9.4716866e-03, -1.9600598e-02],\n",
       "              ...,\n",
       "              [-3.7083837e-01,  3.9503372e-01,  2.1793191e-01, ...,\n",
       "               -7.8333420e-01, -7.7951342e-02,  8.1781417e-02],\n",
       "              [-6.3044024e-01,  9.8087139e-02,  3.7746921e-01, ...,\n",
       "               -9.5245862e-01, -1.1988864e-01, -7.7788278e-02],\n",
       "              [-2.3334654e-01,  3.7051561e-01,  7.7487081e-02, ...,\n",
       "               -4.3369269e-01,  1.5697601e-01, -8.5238041e-03]], dtype=float32),\n",
       "       array([[-0.19250396,  0.5599425 , -0.0275634 , ..., -0.25467318,\n",
       "                0.2556768 , -0.05733736],\n",
       "              [-0.45287707,  0.05194954,  0.48022932, ..., -0.60333514,\n",
       "               -0.7978625 ,  0.14541452],\n",
       "              [-0.05531527,  0.12514924, -0.01614063, ..., -0.05733592,\n",
       "                0.02435799, -0.0199574 ],\n",
       "              ...,\n",
       "              [-0.3345723 ,  0.13967016,  0.25773278, ..., -0.232619  ,\n",
       "               -0.717577  , -0.04277676],\n",
       "              [-0.04637856,  0.8140443 ,  0.7812446 , ..., -1.5372247 ,\n",
       "                0.01518971,  1.0381429 ],\n",
       "              [-0.04637856,  0.8140443 ,  0.7812446 , ..., -1.5372247 ,\n",
       "                0.01518971,  1.0381429 ]], dtype=float32)              ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f12eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_vect, y_train)\n",
    "y_pred = model.predict(X_test_vect)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
